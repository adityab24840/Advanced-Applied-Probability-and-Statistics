theme(text = element_text(size = 14, face = 'bold'),
axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
xlab('') + ylab('Percentage') +
ggtitle('Percentage of NAs across all features')
p
# Chunk 7
# Add NA as a factor level for categorical columns
hData[categorical_cols] = lapply(hData[categorical_cols], function(x){addNA(x)})
# Chunk 8
# Make a histogram of rent values
p = ggplot(data = hData) +
geom_histogram(aes(x = rent, y =after_stat(count)),
breaks = seq(mean(hData$rent)-4*sd(hData$rent), mean(hData$rent)+4*sd(hData$rent), by = 25000),
color = 'black', fill = 'blue') +
labs(x = 'Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of house rents')
p
# Chunk 9
# Build a linear model to predict price per square feet as a function of rent
model = lm(price_per_sqft ~ rent, data = hData)
# Chunk 10
# Make a histogram of transformed rent values
hData['logrent'] = log(hData$rent)
p = ggplot(data = hData) +
geom_histogram(aes(x = logrent, y = ..count..), breaks = seq(mean(hData$logrent)-4*sd(hData$logrent), mean(hData$logrent)+4*sd(hData$logrent), by = 0.5), color = 'black', fill = 'blue') +
labs(x = 'Log Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of log-transformed house rents')
p
# Build a linear model to predict price per square feet as a function of logrent
model = lm(price_per_sqft ~ logrent, data = hData)
summary(model)
# Build a linear model to predict log of price per square feet as a function of logrent
hData$logprice_per_sqft = lm(price_per_sqft ~ logrent, data = hData)$residuals
model = lm(logprice_per_sqft ~ logrent, data = hData)
summary(model)
# Chunk 1
# Load essential libraries
library(ggplot2)
library(dplyr)
library(car)
library(glmnet)
# Chunk 2
# Load the price per sqfts dataset
hData = read.csv('D:/MSIS/Semester 2/Advanced-Applied-Probability-and-Statistics/Codes/Data/houseprices_cleaned.csv',
header = TRUE, stringsAsFactors = FALSE, na.strings = c('NA', 'NaN', 'NULL', 'Not available','Not Available',""))
str(hData)
# Chunk 3
str(hData)
# Chunk 4
# Convert 'locality', 'facing' and 'parking' columns to factors
categorical_cols = c('locality', 'facing', 'parking')
hData[categorical_cols] = lapply(hData[categorical_cols], as.factor)
str(hData)
# Chunk 5
# Continuous columns
continuous_cols = c('area', 'rent', 'bathrooms', 'bhk', 'price_per_sqft')
str(hData)
# Chunk 6
# Fraction of NAs in each column of the data frame
hData_NA = setNames(stack(sapply(hData, function(x){(sum(is.na(x))/length(x))*100}))[2:1], c('Feature','Value'))
p = ggplot(data = hData_NA, aes(x = Feature, y = Value)) +
geom_bar(stat = 'identity', fill = 'steelblue', width = 0.3) +
theme(text = element_text(size = 14, face = 'bold'),
axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
xlab('') + ylab('Percentage') +
ggtitle('Percentage of NAs across all features')
p
# Chunk 7
# Add NA as a factor level for categorical columns
hData[categorical_cols] = lapply(hData[categorical_cols], function(x){addNA(x)})
# Chunk 8
# Make a histogram of rent values
p = ggplot(data = hData) +
geom_histogram(aes(x = rent, y =after_stat(count)),
breaks = seq(mean(hData$rent)-4*sd(hData$rent), mean(hData$rent)+4*sd(hData$rent), by = 25000),
color = 'black', fill = 'blue') +
labs(x = 'Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of house rents')
p
# Chunk 9
# Build a linear model to predict price per square feet as a function of rent
model = lm(price_per_sqft ~ rent, data = hData)
# Chunk 10
# Make a histogram of transformed rent values
hData['logrent'] = log(hData$rent)
p = ggplot(data = hData) +
geom_histogram(aes(x = logrent, y = ..count..), breaks = seq(mean(hData$logrent)-4*sd(hData$logrent), mean(hData$logrent)+4*sd(hData$logrent), by = 0.5), color = 'black', fill = 'blue') +
labs(x = 'Log Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of log-transformed house rents')
p
# Chunk 11
# Build a linear model to predict price per square feet as a function of logrent
model = lm(price_per_sqft ~ logrent, data = hData)
summary(model)
# Build a linear model to predict log of price per square feet as a function of logrent
hData$logprice_per_sqft = lm(price_per_sqft ~ logrent, data = hData)$residuals
model = lm(logprice_per_sqft ~ logrent, data = hData)
summary(model)
# Construct OLS solution obtained using theoretical formulation for predicting price per sqft as a function of continuous predictor logrent and categorical predictor facing
# Response values
y = hData$price_per_sqft
# Design matrix
# Add a column of ones for the intercept
X = model.matrix(~ logrent + facing - 1, data = hData)
# Solve for the coefficient estimates
betahat = solve(t(X) %*% X) %*% t(X) %*% y
print(betahat)
# Chunk 1
# Load essential libraries
library(ggplot2)
library(dplyr)
library(car)
library(glmnet)
# Chunk 2
# Load the price per sqfts dataset
hData = read.csv('D:/MSIS/Semester 2/Advanced-Applied-Probability-and-Statistics/Codes/Data/houseprices_cleaned.csv',
header = TRUE, stringsAsFactors = FALSE, na.strings = c('NA', 'NaN', 'NULL', 'Not available','Not Available',""))
str(hData)
# Chunk 3
str(hData)
# Chunk 4
# Convert 'locality', 'facing' and 'parking' columns to factors
categorical_cols = c('locality', 'facing', 'parking')
hData[categorical_cols] = lapply(hData[categorical_cols], as.factor)
str(hData)
# Chunk 5
# Continuous columns
continuous_cols = c('area', 'rent', 'bathrooms', 'bhk', 'price_per_sqft')
str(hData)
# Chunk 6
# Fraction of NAs in each column of the data frame
hData_NA = setNames(stack(sapply(hData, function(x){(sum(is.na(x))/length(x))*100}))[2:1], c('Feature','Value'))
p = ggplot(data = hData_NA, aes(x = Feature, y = Value)) +
geom_bar(stat = 'identity', fill = 'steelblue', width = 0.3) +
theme(text = element_text(size = 14, face = 'bold'),
axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
xlab('') + ylab('Percentage') +
ggtitle('Percentage of NAs across all features')
p
# Chunk 7
# Add NA as a factor level for categorical columns
hData[categorical_cols] = lapply(hData[categorical_cols], function(x){addNA(x)})
# Chunk 8
# Make a histogram of rent values
p = ggplot(data = hData) +
geom_histogram(aes(x = rent, y =after_stat(count)),
breaks = seq(mean(hData$rent)-4*sd(hData$rent), mean(hData$rent)+4*sd(hData$rent), by = 25000),
color = 'black', fill = 'blue') +
labs(x = 'Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of house rents')
p
# Chunk 9
# Build a linear model to predict price per square feet as a function of rent
model = lm(price_per_sqft ~ rent, data = hData)
# Chunk 10
# Make a histogram of transformed rent values
hData['logrent'] = log(hData$rent)
p = ggplot(data = hData) +
geom_histogram(aes(x = logrent, y = ..count..), breaks = seq(mean(hData$logrent)-4*sd(hData$logrent), mean(hData$logrent)+4*sd(hData$logrent), by = 0.5), color = 'black', fill = 'blue') +
labs(x = 'Log Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of log-transformed house rents')
p
# Chunk 11
# Build a linear model to predict price per square feet as a function of logrent
model = lm(price_per_sqft ~ logrent, data = hData)
summary(model)
# Chunk 12
# Build a linear model to predict log of price per square feet as a function of logrent
hData$logprice_per_sqft = lm(price_per_sqft ~ logrent, data = hData)$residuals
model = lm(logprice_per_sqft ~ logrent, data = hData)
summary(model)
# Build a linear model to predict price per sqft as a function of logrent and facing
model = lm(price_per_sqft ~ logrent + facing, data = hData)
# Construct OLS solution obtained using theoretical formulation for predicting price per sqft as a function of continuous predictor logrent and categorical predictor facing
# Response values
y = hData$price_per_sqft
# Design matrix
# Add a column of ones for the intercept
X = model.matrix(~ logrent + facing - 1, data = hData)
# Solve for the coefficient estimates
betahat = solve(t(X) %*% X) %*% t(X) %*% y
print(betahat)
# Construct OLS solution obtained using theoretical formulation for predicting price per sqft as a function of continuous predictor logrent and categorical predictor facing
# Response values
y = hData$price_per_sqft
# Design matrix
# Add a column of ones for the intercept
X = model.matrix(~ logrent + facing - 1, data = hData)
# Solve for the coefficient estimates
betahat = solve(t(X) %*% X) %*% t(X) %*% y
print(betahat)
# Construct OLS solution obtained using theoretical formulation for predicting price per sqft as a function of continuous predictor logrent and categorical predictor facing
# Response values
y = hData$price_per_sqft
# Design matrix
# Add a column of ones for the intercept
X = model.matrix(~ logrent + facing - 1, data = hData)
# Solve for the coefficient estimates
betahat = solve(t(X) %*% X) %*% t(X) %*% y
print(betahat)
# Construct OLS solution obtained using theoretical formulation for predicting price per sqft as a function of continuous predictor logrent and categorical predictor facing
# Response values
y = hData$price_per_sqft
# Design matrix
# Add a column of ones for the intercept
X = model.matrix(~ logrent + facing - 1, data = hData)
# Solve for the coefficient estimates
betahat = solve(t(X) %*% X) %*% t(X) %*% y
print(betahat)
# Construct OLS solution obtained using theoretical formulation for predicting price per sqft as a function of continuous predictor logrent and categorical predictor facing
# Response values
y = hData$price_per_sqft
# Design matrix
# Add a column of ones for the intercept
X = model.matrix(~ logrent + facing - 1, data = hData)
# Solve for the coefficient estimates
betahat = solve(t(X) %*% X) %*% t(X) %*% y
print(betahat)
# Construct OLS solution obtained using theoretical formulation for predicting price per sqft as a function of continuous predictor logrent and categorical predictor facing
# Response values
y = hData$price_per_sqft
# Design matrix
# Add a column of ones for the intercept
X = model.matrix(~ logrent + facing - 1, data = hData)
# Solve for the coefficient estimates
betahat = solve(t(X) %*% X) %*% t(X) %*% y
print(betahat)
# Chunk 1
# Load essential libraries
library(ggplot2)
library(dplyr)
library(car)
library(glmnet)
# Chunk 2
# Load the price per sqfts dataset
hData = read.csv('D:/MSIS/Semester 2/Advanced-Applied-Probability-and-Statistics/Codes/Data/houseprices_cleaned.csv',
header = TRUE, stringsAsFactors = FALSE, na.strings = c('NA', 'NaN', 'NULL', 'Not available','Not Available',""))
str(hData)
# Chunk 3
str(hData)
# Chunk 4
# Convert 'locality', 'facing' and 'parking' columns to factors
categorical_cols = c('locality', 'facing', 'parking')
hData[categorical_cols] = lapply(hData[categorical_cols], as.factor)
str(hData)
# Chunk 5
# Continuous columns
continuous_cols = c('area', 'rent', 'bathrooms', 'bhk', 'price_per_sqft')
str(hData)
# Chunk 6
# Fraction of NAs in each column of the data frame
hData_NA = setNames(stack(sapply(hData, function(x){(sum(is.na(x))/length(x))*100}))[2:1], c('Feature','Value'))
p = ggplot(data = hData_NA, aes(x = Feature, y = Value)) +
geom_bar(stat = 'identity', fill = 'steelblue', width = 0.3) +
theme(text = element_text(size = 14, face = 'bold'),
axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
xlab('') + ylab('Percentage') +
ggtitle('Percentage of NAs across all features')
p
# Chunk 7
# Add NA as a factor level for categorical columns
hData[categorical_cols] = lapply(hData[categorical_cols], function(x){addNA(x)})
# Chunk 8
# Make a histogram of rent values
p = ggplot(data = hData) +
geom_histogram(aes(x = rent, y =after_stat(count)),
breaks = seq(mean(hData$rent)-4*sd(hData$rent), mean(hData$rent)+4*sd(hData$rent), by = 25000),
color = 'black', fill = 'blue') +
labs(x = 'Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of house rents')
p
# Chunk 9
# Build a linear model to predict price per square feet as a function of rent
model = lm(price_per_sqft ~ rent, data = hData)
# Chunk 10
# Make a histogram of transformed rent values
hData['logrent'] = log(hData$rent)
p = ggplot(data = hData) +
geom_histogram(aes(x = logrent, y = ..count..), breaks = seq(mean(hData$logrent)-4*sd(hData$logrent), mean(hData$logrent)+4*sd(hData$logrent), by = 0.5), color = 'black', fill = 'blue') +
labs(x = 'Log Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of log-transformed house rents')
p
# Chunk 11
# Build a linear model to predict price per square feet as a function of logrent
model = lm(price_per_sqft ~ logrent, data = hData)
summary(model)
# Chunk 12
# Build a linear model to predict log of price per square feet as a function of logrent
hData$logprice_per_sqft = lm(price_per_sqft ~ logrent, data = hData)$residuals
model = lm(logprice_per_sqft ~ logrent, data = hData)
summary(model)
# Chunk 13
# Build a linear model to predict log of price per square feet as a function of rent
model = lm(logprice_per_sqft ~ rent, data = hData)
# Chunk 14
# Build a linear model to predict price per sqft as a function of logrent and facing
model = lm(price_per_sqft ~ logrent + facing, data = hData)
# Load essential libraries
library(ggplot2)
library(dplyr)
library(HSAUR)
library(ggcorrplot)
# Load the heptathlon dataset
data(heptathlon)
str(heptathlon)
# Introduce a new column called sprint highlighting slow and fast sprinters
heptathlon = heptathlon %>% mutate(sprint = ifelse(run200m <= 25 & run800m <= 129, 'fast', 'slow'))
str(heptathlon)
# Change sprint column to factor type
heptathlon['sprint'] = lapply(heptathlon['sprint'], as.factor)
str(heptathlon)
# Make a scatter plot between *run200m* (x-axis) and *longjump* (y-axis). What do you observe from this plot?
p = ggplot(heptathlon, aes(x=run200m,y=longjump))+
geom_point(color='pink',size=4)+  labs(title = "Scatter Plot",x='Run 200m',y='longjump')+
theme_minimal()
p
# Correlation between all pairs of continuous predictors (leave out sprint and the response variable score). What do you observe?
cor_matrix = cor(heptathlon %>% select(-c(sprint, score)))
ggcorrplot(cor_matrix, method = 'circle', lab = TRUE)
#Values close to 1 indicate a strong positive correlation, while values close to -1 indicate a strong negative correlation.
# Make a scatter plot between *run200m* (x-axis) and *longjump* (y-axis) now with the data points color-coded using *sprint*. What do you observe from this plot?
ggplot(heptathlon, aes(x = run200m, y = longjump, color = sprint)) +
geom_point() +
labs(title = "Scatter Plot: run200m vs longjump with Sprint Color Coding",
x = "run200m", y = "longjump") + theme_minimal()
# Calculate Pearson's correlation between *run200m* and *longjump*. What do you observe?
cor2 = cor(heptathlon['run200m'], heptathlon['longjump'], method = "pearson")
cor2
# How many levels does the categorical variable *sprint* have? What is the reference level?
contrasts(heptathlon$sprint)
levels(heptathlon$sprint)
# 2 levels
# Fit a linear model for approximating *score* as a function of *sprint*. Print the model's summary. How accurate is the model? How do the slow athletes' scores compare to the fast ones?
model = lm(data = heptathlon, score ~ sprint)
summary(model)
mean_slow = mean(heptathlon[heptathlon$sprint == 'slow', 'score'])
mean_fast = mean(heptathlon[heptathlon$sprint == 'fast', 'score'])
mean_slow
mean_fast
mean_slow-mean_fast
# Fit a linear model for approximating *score* as a function of *shot* and *sprint*. Print the model's summary and answer the following questions:
# 1. Did the addition of the new predictor *shot* improve the model accuracy?
# 2. *True/false* (explain in one line): the model suggests that there is a possible linear relationship between an athlete's score and shotput performance.
# 3. For a 1 metre increase in shot put throw and with the same sprint performance, we can say with 95% confidence that the athlete's score will increase/decrease by an amount in the interval [?, ?].
model = lm(data = heptathlon,score ~ shot + sprint)
summary(model)
# accuracy increases after adding the feature shot
#true
#[135.236,364.164].
#  Using the model built above, extract the slope and intercept for estimating the *score* of *slow* and *fast* athletes.
# For slow athletes
intercept_slow = 3080.0
slope_slow = 249.7
# For fast athletes
intercept_fast = 2749.6
slope_fast = 249.7
# Complete the code below to build a linear model for approximating *score* as a function of *shot* and *sprint* using the training data. Predict the model performance by applying it to the test data.
# Split the data into 80% train and 20% test parts
set.seed(0)
train_ind = sample(1:nrow(heptathlon), size = 0.8*nrow(heptathlon))
hDataTrain = heptathlon[train_ind, ]
hDataTest = heptathlon[-train_ind, ]
# Build linear regression model
model = lm(score ~ shot + sprint, data = hDataTrain)
# Predict on the test data
predictions = predict(model, newdata = hDataTest)
# Print the true and predicted scores for the test data
print(cbind(TrueScore = hDataTest$score, PredictedScore = predictions))
# Calculate the model error (mean-squared error for test data)
mse = mean((hDataTest$score - predictions)^2)
print(paste("Mean Squared Error: ", mse))
# Fit a linear model for approximating *score* as a function of *shot*, *javelin*, and *sprint*. Print the model's summary and answer the following questions:
#1. Did the addition of the new predictor *javelin* improve the model accuracy?
#2. *True/false* (explain in one line): the model suggests that there is a possible linear relationship between an athlete's score and javelin performance.
#3. For a 1 metre increase in shot put throw and with the same javelin and sprint performance, we can say with 95% confidence that the athlete's score will increase/decrease by an amount in the interval [?, ?].
model =  lm(score ~ shot + javelin + sprint, data = hDataTrain)
summary(model)
# there is no significant increase in accuracy
# false
#[137.37,417.93]
# Fit a linear model for approximating *score* as a function of *highjump*, and *sprint*. Print the model's summary and answer the following questions:
# 1. How accurate is this model?
# 2. Considering a p-value of 10% as cutoff, are there any insignificant features?
model = lm(data = hDataTrain,score ~ highjump + sprint)
summary(model)
#Multiple R-squared:  0.8369,	Adjusted R-squared:  0.8177
#no
# Chunk 1
library(ggplot2)
library(dplyr)
library(reshape)
# Chunk 2
# Load the diabetes dataset:
# 10 predictors which are age, gender (1-female, 2-male), body-mass index, average blood pressure, and six blood serum measurements and 1 response variable which is a quantitative measure of disease progression one year after baseline)
df = read.csv('diabetes.csv', header = TRUE, stringsAsFactors = FALSE)
# Chunk 1
library(ggplot2)
library(dplyr)
library(reshape)
# Chunk 2
# Load the diabetes dataset:
# 10 predictors which are age, gender (1-female, 2-male), body-mass index, average blood pressure, and six blood serum measurements and 1 response variable which is a quantitative measure of disease progression one year after baseline)
df = read.csv('diabetes.csv', header = TRUE, stringsAsFactors = FALSE)
setwd("D:/MSIS/Semester 2/Advanced-Applied-Probability-and-Statistics/Assignment")
# Chunk 1
library(ggplot2)
library(dplyr)
library(reshape)
# Chunk 2
# Load the diabetes dataset:
# 10 predictors which are age, gender (1-female, 2-male), body-mass index, average blood pressure, and six blood serum measurements and 1 response variable which is a quantitative measure of disease progression one year after baseline)
df = read.csv('diabetes.csv', header = TRUE, stringsAsFactors = FALSE)
str(df)
# Chunk 3
# Create a new feature called BMILEVEL using the BMI column and the following rules: BMI < 18.5 is underweight, 18.5 <= BMI <= 24.9 is healthy, 25 <= BMI <= 29.9 is overweight, BMI >= 30 is unhealthy
df = df %>% mutate(BMILEVEL = case_when(BMI < 18.5 ~ 'underweight', BMI >= 18.5 & BMI <= 24.9 ~ 'healthy', BMI >= 25 & BMI <= 29.9 ~ 'overweight', BMI >= 30 ~ 'unhealthy'))
str(df)
# Chunk 4
# Convert 'GENDER' and 'BMILEVEL' columns to factors
categorical_cols = c('GENDER', 'BMILEVEL')
df[categorical_cols] = lapply(df[categorical_cols], as.factor)
str(df)
# Chunk 5
# Create a list of continuous columns
continuous_cols = setdiff(colnames(df), categorical_cols)
continuous_cols
# Chunk 6
# How many levels does the categorical variable *BMILEVEL* have? What is the reference level?
contrasts(df$BMILEVEL)
levels(df$BMILEVEL)
# Chunk 7
# Fit a linear model for predicting disease progression using BMILEVEL. Print the model's summary.
# How accurate is the model?
# Which level in BMILEVEL is most likely to not have a linear relationship with disease progression? What is the reason?
# How worse is the disease progression in unhealthy people compared to the healthy ones?
# How worse is the disease progression in unhealthy people compared to the overweight ones?
# Write down the individual model for each level in BMILEVEL
# Fit a linear model for predicting disease progression using BMILEVEL. Print the model's summary.
model <- lm( Y  ~ BMILEVEL, data = df)
summary(model)
# Chunk 8
The model using only BMI levels explains about 28.32% of the variation in disease progression. It suggests that being "underweight" might not strongly affect disease progression, as its impact isn't clear and can vary a lot.
# Chunk 1
library(ggplot2)
library(dplyr)
library(reshape)
# Chunk 2
# Load the diabetes dataset:
# 10 predictors which are age, gender (1-female, 2-male), body-mass index, average blood pressure, and six blood serum measurements and 1 response variable which is a quantitative measure of disease progression one year after baseline)
df = read.csv('diabetes.csv', header = TRUE, stringsAsFactors = FALSE)
str(df)
# Chunk 3
# Create a new feature called BMILEVEL using the BMI column and the following rules: BMI < 18.5 is underweight, 18.5 <= BMI <= 24.9 is healthy, 25 <= BMI <= 29.9 is overweight, BMI >= 30 is unhealthy
df = df %>% mutate(BMILEVEL = case_when(BMI < 18.5 ~ 'underweight', BMI >= 18.5 & BMI <= 24.9 ~ 'healthy', BMI >= 25 & BMI <= 29.9 ~ 'overweight', BMI >= 30 ~ 'unhealthy'))
str(df)
# Chunk 4
# Convert 'GENDER' and 'BMILEVEL' columns to factors
categorical_cols = c('GENDER', 'BMILEVEL')
df[categorical_cols] = lapply(df[categorical_cols], as.factor)
str(df)
# Chunk 5
# Create a list of continuous columns
continuous_cols = setdiff(colnames(df), categorical_cols)
continuous_cols
# Chunk 6
# How many levels does the categorical variable *BMILEVEL* have? What is the reference level?
contrasts(df$BMILEVEL)
levels(df$BMILEVEL)
# Chunk 7
# Fit a linear model for predicting disease progression using BMILEVEL. Print the model's summary.
# How accurate is the model?
# Which level in BMILEVEL is most likely to not have a linear relationship with disease progression? What is the reason?
# How worse is the disease progression in unhealthy people compared to the healthy ones?
# How worse is the disease progression in unhealthy people compared to the overweight ones?
# Write down the individual model for each level in BMILEVEL
# Fit a linear model for predicting disease progression using BMILEVEL. Print the model's summary.
model <- lm( Y  ~ BMILEVEL, data = df)
summary(model)
# Chunk 8
The model using only BMI levels explains about 28.32% of the variation in disease progression. It suggests that being "underweight" might not strongly affect disease progression, as its impact isn't clear and can vary a lot.
