---
title: "Linear Regression Coding Assignment-1"
editor_options:
  chunk_output_type: console
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

```{r}
# Load essential libraries
library(ggplot2)
library(dplyr)
```

```{r}
# Load the house price dataset
hData = read.csv('houseprices_cleaned.csv')
str(hData)
```

```{r}
# Convert 'locality', 'facing' and 'parking' columns to factors
categorical_cols = c('locality','facing','parking')
hData[categorical_cols] = lapply(hData[categorical_cols],as.factor)
str(hData)
```

```{r}
# Continuous columns
continuous_cols = setdiff(colnames(hData),categorical_cols)
```

```{r}
# Plot percentage of NAs in each column of the data frame
hData_NA = setNames(stack(sapply(hData, function(x){(sum(is.na(x))/length(x))*100}))[2:1], c('Feature','Value'))
p = ggplot(data = hData_NA, aes(x = Feature, y = Value)) +
  geom_bar(stat = 'identity', fill = 'steelblue', width = 0.3) +
  theme(text = element_text(size = 14, face = 'bold'),
  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  xlab('') + ylab('Percentage') +
  ggtitle('Percentage of NAs across all features')
p
```

```{r}
# Add NA as a factor level for categorical columns 

hData[categorical_cols] = lapply(hData[categorical_cols],addNA)
str(hData)
```

```{r}
# Make a histogram of rent values
p = ggplot(data = hData) +
  geom_histogram(aes(x = rent, y = after_stat(count)), breaks = seq(mean(hData$rent)-4*sd(hData$rent), mean(hData$rent)+4*sd(hData$rent), by = 25000), color = 'black', fill = 'blue') +
  labs(x = 'Rent', y = 'Frequency')  +
   theme(axis.text = element_text(size = 8),
   axis.text.x = element_text(size = 10),
   axis.text.y = element_text(size = 10),
   axis.title = element_text(size = 10, face = "bold"))  +
  ggtitle('Histogram of house rents')
p
```

```{r}
# Build a linear model to predict price per square feet as a function of rent. How accurate is the model?
model = lm(data = hData,price_per_sqft ~ rent)
summary(model)
```

```{r}
# Make a histogram of log-transform
hData['logrent'] = log(hData['rent'])
p = ggplot(data = hData) +
  geom_histogram(aes(x = logrent, y = after_stat(count)), breaks = seq(mean(hData$logrent)-4*sd(hData$logrent), mean(hData$logrent)+4*sd(hData$logrent), by = 0.5), color = 'black', fill = 'blue') +
  labs(x = 'Rent', y = 'Frequency')  +
   theme(axis.text = element_text(size = 8),
   axis.text.x = element_text(size = 10),
   axis.text.y = element_text(size = 10),
   axis.title = element_text(size = 10, face = "bold"))  +
  ggtitle('Histogram of house rents')
p
```

```{r}
# Build a linear model to predict price per square feet as a function of logrent. Did log-transforming rent help improve the model accuracy?
model =  lm(data = hData,price_per_sqft ~ logrent)
summary(model)
print('accuracy increases')
```

```{r}
# Build a linear model to predict log of price per square feet as a function of logrent. Did log-transforming the response variable price per square feet improve the model accuracy?
hData['logprice_per_sqft'] = log(hData['price_per_sqft'])
model = lm(data = hData,logprice_per_sqft ~ logrent)
summary(model)
#accuracy decreases but barely
```

```{r}
# Build a linear model to predict sqrt of price per square feet as a function of logrent. Did sqrt-transforming the response variable price per square feet improve the model accuracy?
hData['sqrtprice_per_sqft'] = sqrt(hData['price_per_sqft'])
model = lm(data = hData,sqrtprice_per_sqft ~ logrent)
summary(model)
```

```{r}
# Build a linear model to predict price per sqft as a function of area and rent. Did adding area as an additional predictor improve model accuracy (compared to only rent as the predictor)? Also, interpret the coefficient estimates for area and rent practically.
model = lm(data = hData, price_per_sqft ~ area + rent )
summary(model)
#the coefficents for rent and area mean that for unint increase in rent or area keeping everying else constant the change in the predicted price per sqft
#the model is more accurate comapred to only rent as a predictor
```

```{r}
# Build a linear model to predict sqrt of price per sqft as a function of area and logrent. Did adding area as an additional predictor improve model accuracy (compared to only logrent as the predictor)? Also, interpret the coefficient estimates for area and logrent practically.
model = lm(data = hData,sqrtprice_per_sqft ~ area + logrent)
summary(model)
#the model is more accurate 
```

```{r}
# Build a linear model to predict sqrt of price per sqft as a function of logarea and logrent. Did log-transforming area improve model accuracy?
hData['logarea'] = log(hData['area'])
model = lm(data = hData,sqrtprice_per_sqft ~ logarea + logrent )
summary(model)
#the model is the most accurate among the rest
```

```{r}
# Build a linear model to predict price per sqft as a function of area, rent, and parking (compared to just using area and rent as predictors). Did adding parking as an additional predictor improve model accuracy?
model = lm(data = hData,price_per_sqft ~ area + rent + parking )
summary(model)
#the model is just as accurate even after adding parking

```

```{r}
# Build a linear model to predict sqrt of price per sqft as a function of logarea, logrent, and locality. Did adding locality as an additional predictor improve model accuracy (compared to just using logarea and logrent as predictors)?
model = lm(data = hData,sqrtprice_per_sqft ~ logarea + logrent + locality )
summary(model)
#the model is slightly more accuarate than just using logrent and logarea

```

```{r}
# Build a linear model to predict price per sqft as a function of area, rent, and parking. How many levels does the categorical feature parking have? How many new variables are introduced for the categorical variable parking? Interpret all regression coefficient estimates except the intercept coefficient estimate beta0 practically. Do the p-values suggest any insignificant features (that is, features which probably don't have a linear relationship with the response variable?
model = lm(data = hData,price_per_sqft ~ area + rent + parking )
summary(model)
#there are 3 levels
#since the ep values are not close to 0 it might indicate non linear relationship
```

```{r}
# Create new columns corresponding to scaled versions of the continuous columns
hData[paste0('scaled_', continuous_cols)] = lapply(hData[continuous_cols], scale)
str(hData)
```

```{r}
# Build a linear model to predict scaled price per sqft as a function of scaled area and scaled rent. Compare this with the model built using unscaled data: that is, predict price per sqft as a function of area and rent. Does scaling help?
model_scaled = lm(data = hData, scaled_price_per_sqft ~ scaled_area + scaled_rent)
summary(model_scaled)
#scaling doesnt help in this case
```

```{r}
# Rebuild a linear model to predict sqrt of price per sqft as a function of logarea, logrent, and locality which we will evaluate using a train-test split of the dataset
model = lm(data = hData, sqrt(price_per_sqft) ~ logarea + logrent + locality)
summary(model)
```

```{r}
# Split data into train (80%) and test (20%) sets and evaluate model performance on train and test sets. Run this cell multiple times for a random splitting of the data into train and test sets and report the model performance on the resulting train and test sets. Is there much variability in the model performance across different test sets? If that is the case, then the model is not generalizing well and is overfitting the train set. Is it the case here?
set.seed(123)  
ind = sample(nrow(hData), size = floor(0.8*nrow(hData)), replace = FALSE)
hData_train = hData[ind, ]
hData_test = hData[-ind, ]
# Calculate RMSE (root-mean-squared-error) on train data
train_error = sqrt(mean((hData_train$price_per_sqft - predict(model, hData_train))^2))

# Calculate RMSE (root-mean-squared-error) on test data
test_error = sqrt(mean((hData_test$price_per_sqft - predict(model, hData_test))^2))

print(train_error)
print(test_error)
#there isnt overfitting both the train and test modelperfom close to the same 
```
